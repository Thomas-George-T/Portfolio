+++
# Experience widget.
widget = "experience"  # See https://sourcethemes.com/academic/docs/page-builder/
headless = true  # This file represents a page section.
active = true  # Activate this widget? true/false
weight = 40  # Order that this section will appear.

title = "Experience"
#subtitle = "PREVIOUS ASSOCIATIONS THAT HELPED TO GATHER EXPERIENCE"

# Date format for experience
#   Refer to https://sourcethemes.com/academic/docs/customization/#date-format
date_format = "Jan 2006"

# Experiences.
#   Add/remove as many `[[experience]]` blocks below as you like.
#   Required fields are `title`, `company`, and `date_start`.
#   Leave `date_end` empty if it's your current employer.
#   Begin/end multi-line descriptions with 3 quotes `"""`.
[[experience]]
  title = "Senior Software Engineer - Niche"
  company = "Legato Health Technologies, Anthem Inc."
  company_url = ""
  location = "Bangalore, India"
  date_start = "2018-06-04"
  date_end = ""
  description = """
  Highlights :
  
* Engaged primarily in developing Spark Scala code involving RDDâ€™s, dataframes, and SparkSQL.
* Developed shell scripts to process 1.5 TB CSV, Parquet data from inbound to the outbound layer for generating Tableau reports.
* Developed CI/CD pipelines using Bamboo to build & deploy scripts and ETL jars into pre-prod and prod environments to reduce manual effort.

Innovations :

* Improved runtime from 18 hours to 9.5 hours by refactoring Spark Scala ETL code.
* Refactored tables to use parquet formats, snappy compressions, and include dynamic partitions. 
* Improved efficiency and turnaround time from 6 hours to 1.5 hours by automating data quality and validity checks between Hive and SQL server loads.
* Developed SIT automation scripts in Spark Scala to assess quality, validity, counts of inbound data files & tables to remove manual effort and intervention.
  """

[[experience]]
  title = "Software Engineer - Big Data"
  company = "Middle East Management Consultancy and Marketing"
  company_url = ""
  location = "Muscat, Oman"
  date_start = "2016-06-01"
  date_end = "2018-05-31"
  description = """
Highlights :
* Shipped and delivered product end to end.
* Implemented SQOOP for massive dataset transfer between the Hadoop file system and RDBMS.
* Involved in the design and creation of partitioned table DDLs in Hive.
* Worked on performance tuning, analysis, and response time reduction techniques in SQL and Sqoop.
* Worked with different file formats such as JSON, CSV, Parquet, ORC, and snappy compressed files.
* Processed delimited data using Spark SQL to build pipelines from landing zone to outbound layer.
* Created market trend reports analyzed from aggregated Flume data improving sales by 9% annually.
  """

[[experience]]
  title = "Practice School Student/ Researcher"
  company = "Manipal Institute of Technology"
  company_url = ""
  location = "Manipal, India"
  date_start = "2016-01-01"
  date_end = "2016-05-19"
  description = """Central Data Repository for MIT, Manipal: 
  
  Delivered a web application with its main objectives to serve as a means of data entry, to collect the required data, to analyze the given data, and finally to generate reports dynamically according to the custom report format requirements of the user. The data was loaded from the databases using Sqoop and analyzed using a Hadoop cluster. The reports are generated after querying using Hive and displayed in the web application."""
  
[[experience]]
  title = "Software Development Intern"
  company = "CGI Information Systems and Management Consultants Pvt. Ltd"
  company_url = ""
  location = "Manipal, India"
  date_start = "2015-05-01"
  date_end = "2015-07-30"
  description = """Project Management System: 
  
  Developed a web application that enabled the interaction between different users of different departments and their respective projects while accessing their functions on a large scale."""  
+++
